Homework 3 Solutions
================
Laura Gomez

## Problem 1

``` r
data("instacart")
```

This data set contains 1384617 rows and … columns.

Observations are the level of items in orders by user. There are
user/order variables – user ID, order ID, order day, and order hour.
There are also item variables – name, aisle, department, and some
numeric codes.

How many aisles are there, and which aisles are the most items ordered
from?

``` r
instacart %>%
  count(aisle) %>%
  arrange(desc(n))
```

    ## # A tibble: 134 x 2
    ##    aisle                              n
    ##    <chr>                          <int>
    ##  1 fresh vegetables              150609
    ##  2 fresh fruits                  150473
    ##  3 packaged vegetables fruits     78493
    ##  4 yogurt                         55240
    ##  5 packaged cheese                41699
    ##  6 water seltzer sparkling water  36617
    ##  7 milk                           32644
    ##  8 chips pretzels                 31269
    ##  9 soy lactosefree                26240
    ## 10 bread                          23635
    ## # … with 124 more rows

Let’s make a plot

``` r
instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

<img src="p8105_hw3_lcg2154_files/figure-gfm/unnamed-chunk-3-1.png" width="90%" />

Make a table \!

``` r
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(aisle, rank) %>%
  knitr::kable()
```

| aisle                      | product\_name                                 |    n | rank |
| :------------------------- | :-------------------------------------------- | ---: | ---: |
| baking ingredients         | Light Brown Sugar                             |  499 |    1 |
| baking ingredients         | Pure Baking Soda                              |  387 |    2 |
| baking ingredients         | Cane Sugar                                    |  336 |    3 |
| dog food care              | Snack Sticks Chicken & Rice Recipe Dog Treats |   30 |    1 |
| dog food care              | Organix Chicken & Brown Rice Recipe           |   28 |    2 |
| dog food care              | Small Dog Biscuits                            |   26 |    3 |
| packaged vegetables fruits | Organic Baby Spinach                          | 9784 |    1 |
| packaged vegetables fruits | Organic Raspberries                           | 5546 |    2 |
| packaged vegetables fruits | Organic Blueberries                           | 4966 |    3 |

Apples vs. ice cream..

``` r
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```

    ## `summarise()` regrouping output by 'product_name' (override with `.groups` argument)

    ## # A tibble: 2 x 8
    ## # Groups:   product_name [2]
    ##   product_name       `0`   `1`   `2`   `3`   `4`   `5`   `6`
    ##   <chr>            <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>
    ## 1 Coffee Ice Cream  13.8  14.3  15.4  15.3  15.2  12.3  13.8
    ## 2 Pink Lady Apples  13.4  11.4  11.7  14.2  11.6  12.8  11.9

## Problem 2

Import and clean the accelerometer dataset. After data is tidy, add the
*weekday vs weekend* variable and encode data with reasonable variable
classes.

``` r
accelerometer_df = read_csv("./Data/accel_data.csv") %>%
  janitor::clean_names() %>%
  drop_na()  %>%
  pivot_longer(activity_1:activity_1440, names_to = 'time', names_prefix = 'activity_', values_to = 'activity_count') %>% 
  mutate(
    day = as.factor(day), 
    time = as.numeric(time)) %>% 
  mutate(
    weekend = ifelse(day == c("Saturday", "Sunday"),1,0), 
    weekend = case_when(weekend == 1 ~ "weekend", weekend == 0 ~ "weekday" )) %>%
  mutate(
    day = forcats::fct_relevel(
      day, "Monday", "Tuesday","Wednesday", "Thursday", "Friday", "Saturday", "Sunday" ))
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_double(),
    ##   day = col_character()
    ## )

    ## See spec(...) for full column specifications.

**Accelerometers Data Description**: The Accelerometers data set
contains device obtained information measuring activity within a one
minute interval for a 24 hour cycle starting at midnight. The following
variables exist in the data set activity\_count, day, day\_id, time,
week, weekend. There are a total of 50400 rows and 6 columns in our
final data set.

Using your tidied data set, aggregate across minutes to create a total
activity variable for each day, and create a table showing these totals.
Are any trends apparent?

``` r
 accelerometer_df %>% 
  group_by(week,day) %>% 
  summarize(total_activity_df = sum(activity_count, na.rm = TRUE)) %>% 
pivot_wider(names_from = "day", 
  values_from = "total_activity_df") %>%
  knitr::kable() 
```

    ## `summarise()` regrouping output by 'week' (override with `.groups` argument)

| week |    Monday |  Tuesday | Wednesday | Thursday |   Friday | Saturday | Sunday |
| ---: | --------: | -------: | --------: | -------: | -------: | -------: | -----: |
|    1 |  78828.07 | 307094.2 |    340115 | 355923.6 | 480542.6 |   376254 | 631105 |
|    2 | 295431.00 | 423245.0 |    440962 | 474048.0 | 568839.0 |   607175 | 422018 |
|    3 | 685910.00 | 381507.0 |    468869 | 371230.0 | 467420.0 |   382928 | 467052 |
|    4 | 409450.00 | 319568.0 |    434460 | 340291.0 | 154049.0 |     1440 | 260617 |
|    5 | 389080.00 | 367824.0 |    445366 | 549658.0 | 620860.0 |     1440 | 138421 |

``` r
# Talk more about trends
```

**Total Activity Data Trend**: Based on the table above, there seems to
be a trend in increase of activity on weekends in the beginning weeks
(week 1 and 2). Then the values begin to increase significantly during
the weekdays in subsequent weeks. Overall there is indirect proportional
relationship between weekdays and weekend: as activity count increases
during the weekends, the activity count of weekdays decreases and
vice-versa.

Make a single-panel plot that shows the 24-hour activity time courses
for each day and use color to indicate day of the week. Describe in
words any patterns or conclusions you can make based on this graph.

``` r
accelerometer_df %>%
  ggplot(aes(x = time, y = activity_count, color = day)) + 
  geom_line(size = 1.5) +
  #geom_smooth(aes(group = day), se = FALSE) +
  labs(
    title = "24hr activity time colored by day of the week",
    x = " Activity time (minutes)",
    y = "Activity Count ")
```

<img src="p8105_hw3_lcg2154_files/figure-gfm/unnamed-chunk-6-1.png" width="90%" />

``` r
#fix by aesthetics, add more transparency 
# talk more about trends
```

**Description** Based on the single-panel plot above, we observe a
greater activity count on days like Saturday and Sunday. The overall
activity count seems to increase for most days, including weekdays,
later in the 24-hour activity cycle around minute 1400.

## Problem 3

Import data

``` r
library(p8105.datasets)
data("ny_noaa")
```

Write a short description of the data set, noting the size and structure
of the data, describing some key variables, and indicating the extent to
which missing data is an issue.

``` r
ny_noaa_df = ny_noaa %>% separate('date', c( 'year','month','day')) %>% 
  mutate(
    day = factor(day), 
    prcp =  prcp/10,
    prcp = as.integer(prcp), 
    tmax = as.numeric(tmax), 
    tmin = as.numeric(tmin), 
    tmax = tmax/10,
    tmin = tmin/10) %>% 
  drop_na()
# for snowfall what are the most commonly observed values? do count from first problem
```

The data set drew information from the National Oceanic and Atmospheric
Association (NOAA) to report on 5 variables, day, id, month, prcp, snow,
snwd, tmax, tmin, year, for New York State Weather Stations from January
1, 1981 through December 31, 2010. There are a total of 1222433 rows and
`rncol(ny_noaa_df)` columns in our final dataset.

Make a two-panel plot showing the average max temperature in January and
in July in each station across years. Is there any observable /
interpretable structure? Any outliers?

``` r
Jan_July_averagemax_df =  ny_noaa_df  %>% 
  group_by(month, year, id) %>% 
  filter(month %in% c("01","07")) %>% 
  summarize(average_tmax = mean(tmax)) %>% 
  mutate( year = as.numeric(year)) %>%
  ggplot(aes(x = year, y = average_tmax, group = id)) +
  geom_point() +
  geom_path() +
  facet_grid(~ month) +
  labs( 
     title = "Mean average temperature for January and July across stations and years", 
      x = "year", 
     y = "average max temperature(C)")
```

    ## `summarise()` regrouping output by 'month', 'year' (override with `.groups` argument)

``` r
#structure is conserving between stations, one year warmer than other
```

Make a two-panel plot showing (i) tmax vs tmin for the full dataset
(note that a scatterplot may not be the best option)

``` r
tmax_tmin_df =
  ny_noaa_df %>%
  ggplot(aes(x = tmin, y = tmax)) + geom_hex()
```

2)  make a plot showing the distribution of snowfall values greater than
    0 and less than 100 separately by year.

<!-- end list -->

``` r
snowfall_df = 
  ny_noaa_df %>%
  drop_na() %>%
  filter( 'snow' > 0, 'snow' < 100 ) %>%
  ggplot(aes(x = year , y = snow, color = year)) +
  geom_boxplot() +
  labs( x = "Year", 
        y = "Snowfall Values (mm)")
```
