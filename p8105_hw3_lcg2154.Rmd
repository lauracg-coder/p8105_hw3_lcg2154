---
title: "Homework 3 Solutions"
author: Laura Gomez
output: github_document
---

```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(dplyr)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = 0.6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

```{r}
data("instacart")
```

This data set contains `r nrow(instacart)` rows and ... columns. 

Observations are the level of items in orders by user. There are user/order variables -- user ID, order ID, order day, and order hour. There are also item variables -- name, aisle, department, and some numeric codes. 

How many aisles are there, and which aisles are the most items ordered from?
```{r}
instacart %>%
  count(aisle) %>%
  arrange(desc(n))
```

Let's make a plot 
```{r}
instacart %>%
  count(aisle) %>%
  filter(n > 10000) %>%
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>%
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

Make a table !
```{r}
instacart %>%
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>%
  group_by(aisle) %>%
  count(product_name) %>%
  mutate(rank = min_rank(desc(n))) %>%
  filter(rank < 4) %>%
  arrange(aisle, rank) %>%
  knitr::kable()
```

Apples vs. ice cream..
```{r}
instacart %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>%
  summarize(mean_hour = mean(order_hour_of_day)) %>%
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  )
```


## Problem 2

Import and clean the accelerometer dataset. After data is tidy, add the *weekday vs weekend* variable and encode data with reasonable variable classes.
```{r accelerometer_tidy_df}
accelerometer_df = read_csv("./Data/accel_data.csv") %>%
  janitor::clean_names() %>%
  drop_na()  %>%
  pivot_longer(activity_1:activity_1440, names_to = 'time', names_prefix = 'activity_', values_to = 'activity_count') %>% 
  mutate(
    day = as.factor(day), 
    time = as.numeric(time)) %>% 
  mutate(
    weekend = ifelse(day == c("Saturday", "Sunday"),1,0), 
    weekend = case_when(weekend == 1 ~ "weekend", weekend == 0 ~ "weekday" )) %>%
  mutate(
    day = forcats::fct_relevel(
      day, "Monday", "Tuesday","Wednesday", "Thursday", "Friday", "Saturday", "Sunday" ))
```
The Accelerometers data set contains device obtained information measuring activity within a one minute interval for a 24 hour cycle starting at midnight.  The following variables exist in the data set `r ls(accelerometer_df)`. There are a total of `r nrow(accelerometer_df)` rows and `r ncol(accelerometer_df)` columns in our final data set. 

Using your tidied data set, aggregate across minutes to create a total activity variable for each day, and specify apparent trends. 
```{r total activity_df}
 accelerometer_df %>% 
  group_by(week,day) %>% 
  summarize(total_activity_df = sum(activity_count, na.rm = TRUE)) %>% 
pivot_wider(names_from = "day", 
  values_from = "total_activity_df") %>%
  knitr::kable() 
```
Based on the table above, there seems to be a trend in increase of activity on weekends in the beginning weeks (week 1 and 2). Then the values begin to increase significantly during the weekdays in subsequent weeks especially week 5. Overall there is indirect proportional relationship between weekdays and weekend: as activity count increases during the weekends, the activity count of weekdays decreases and vice-versa. 

Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.
```{r}
accelerometer_df %>%
  ggplot(aes(x = time, y = activity_count, color = day)) + 
  geom_line(size = 1.5) +
  labs(
    title = "24hr activity time colored by day of the week",
    x = " Activity time (minutes)",
    y = "Activity Count ")
```
Based on the single-panel plot above, we observe a greater activity count on days like Saturday and Sunday. The overall activity count seems to increase for most days, including weekdays, later in the 24-hour activity cycle around minute 1400. The activity increase in the beginning and end of the activity time which accurately represents mornings and nighttime that generally involve less activity. 

## Problem 3

Import data
```{r}
library(patchwork)
library(ggridges)
library(p8105.datasets)
data("ny_noaa")
```

Write a short description of the data set, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue.
```{r}
ny_noaa_df = ny_noaa %>% separate('date', c( 'year','month','day')) %>% 
  mutate(
    day = factor(day), 
    prcp =  prcp/10,
    prcp = as.integer(prcp), 
    tmax = as.numeric(tmax), 
    tmin = as.numeric(tmin), 
    tmax = tmax/10,
    tmin = tmin/10) %>% 
  drop_na()
```

As specified by the code below, the most commonly observed values are values of high snow volume, measured in mm, as New York City get a large amount of snow. 
```{r}
snow_rank_df = 
  ny_noaa_df %>%
  count(snow, name = "snow_values") %>%
  mutate(rank = min_rank(snow_values)) %>%
  arrange(rank)
```

The data set drew information from the National Oceanic and Atmospheric Association (NOAA) to report on 5 variables, `r ls(ny_noaa_df)`, for New York State Weather Stations from January 1, 1981 through December 31, 2010. There are a total of `r nrow(ny_noaa_df)` rows and `r ncol(ny_noaa_df)` columns in our final data set. 

Make a two-panel plot showing the average max temperature in January and in July in each station across years.
```{r}
Jan_July_averagemax_df =  ny_noaa_df  %>% 
  group_by(month, year, id) %>% 
  filter(month %in% c("01","07")) %>% 
  summarize(average_tmax = mean(tmax)) %>% 
  mutate( year = as.numeric(year)) %>%
  ggplot(aes(x = year, y = average_tmax, group = id)) +
  geom_point() +
  geom_path() +
  facet_grid(~ month) +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  labs( 
     title = "Mean average temperature for January and July across stations", 
      x = "year", 
     y = "average max temperature(C)")

```
The two panel plot above shows a significant difference in the average maximum temperatures in January and July. As expected the average maximum temperatures in January do not exceed 15 degrees Celsius as it is winter in New York. There are no significant outliters the above data set. However, there is an increase in average temperature in later years during the month of January.The average temperature over the years in July seems to stay within a specific range of 20-30 degrees Celsius. 

Make a two-panel plot showing the **t-max** vs **t-min** for the full data set 
```{r}
tmax_tmin_df =
  ny_noaa_df %>%
  ggplot(aes(x = tmin, y = tmax)) + 
  geom_hex() +
  labs( 
     title = "Two-Panel Plot of the Maximum and Minimum Temperatures", 
      x = "Minimum Temperatures (C)", 
     y = "Maximum Temperature(C)")
```
Make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.
```{r}
snowfall_df = 
  ny_noaa_df %>%
  filter( snow > 0, snow < 100 ) %>%
  ggplot(aes(x = snow , y = as.factor(year))) +
  geom_density_ridges(alpha = 1, adjust = 1) +
  theme(axis.text.x = element_text(angle = 20, vjust = 1, hjust = 1)) +
  labs( 
     title = "Snowfall Distribution", 
      x = "Snowfall (mm)", 
     y = "Year") +
  scale_x_continuous(breaks = c(0, 25, 50, 75, 100, 125, 150, 175))
 
tmax_tmin_df / snowfall_df
```



